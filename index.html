<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tian-Ao(Teo) Ren</title>
  
  <meta name="author" content="Tian-Ao Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
  <style>
    p, div {
      text-align: justify;
    }
  </style>
</head>

<body onload="showSelected()">
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:68.5%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tian-Ao(Teo) Ren / 任 天 傲</name>
              </p>
              <p>
                I am a PhD candidate in Mechanical Engineering at the
                <a href="https://bdml.stanford.edu/pmwiki/pmwiki.php/Main/HomePage">Biomimetics & Dexterous Manipulation Laboratory (BDML)</a> at Stanford University, advised by
                <a href="https://scholar.google.com/citations?user=qIg8KFYAAAAJ&hl=en&oi=ao">Prof. Mark Cutkosky</a>. During my master's program, I also worked at the <a href="https://charm.stanford.edu/Main/HomePage">Collaborative Haptics and Robotics in Medicine (CHARM) Lab</a>, advised by
                <a href="https://scholar.google.com/citations?user=lD4Yjn4AAAAJ&hl=en&oi=ao">Prof. Allison Okamura</a>.
                <p>
                I received my M.S. degree in Mechanical Engineering from Stanford University in 2025 and my B.Eng. degree in Robotics Engineering from Beijing University of Chemical Technology (BUCT) in 2023.
              </p>
              <p>
                I was a research assistant at The Chinese University of Hong Kong (CUHK) from 2022 to 2024, working with
              <a href="https://scholar.google.com/citations?user=t77Gv8kAAAAJ&hl=en">Prof. Jiewen Lai</a> and
              <a href="https://scholar.google.com/citations?user=rcF7N44AAAAJ&hl=en">Prof. Hongliang Ren</a>.
              </p>
              <p>I care deeply about building a welcoming and inclusive research community. If you'd like to chat about research, career paths, or anything else, feel free to reach out—especially if you're from an underrepresented group in STEM. I'm always happy to connect and help where I can.
                </p>
              <div style="text-align: center;">
                <span>Email: tianao [at] stanford.edu</span>
              </div>
              <p style="text-align:center">
                <a href="data/teo_cv.pdf">C.V.</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=9UbQA4IAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/teoren">Linkedin</a>  
              </p>
            </td>
            <td style="padding:2.5%;width:31.5%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/Stanford_Teo.jpg" onmouseover="this.src='images/teo_jojo_stand.png'" onmouseout="this.src='images/teo_jojo.png'">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align: justify">
              <heading>News</heading>
              <ul>
                <!-- <li>[Jul 2025] "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types" is accepted by CoRL'25.</li>
                <li>[Jun 2025] "SLIM: A Symmetric, Low-Inertia Manipulator for Constrained, Contact-Rich Spaces" is accepted by RA-L.</li> -->
                <li>[Dec 2025] "Gravity-Aware Proactive Joint-Level Compensation for Portable Soft Slender Robots Using A Single IMU and Real-Time Simulation" is accepted by <strong>IJRR</strong>.</li>
                <!-- <li>[Apr 2025] "Whisker-Inspired Tactile Sensing: A Sim2Real Approach for Precise Underwater Contact Tracking" is accepted by RA-L.</li>
                <li>[Nov 2024] We presented our whikser-inspired sensors with OceanOneK at <a href="https://src.stanford.edu/demo/underwater-whisker-sensing">Stanford Robotics Center Launch</a></li>
                <li>[Oct 2024] "Grasp as You Say: Language-guided Dexterous Grasp Generation" is accepted by NeurIPS'24.</li> -->
                <li>[Oct 2025] Passed my PhD Qualifying Exam! </li>
                <!-- <li>[Jun 2023] "The Design of a Virtual Prototyping System for Authoring Interactive VR Environments from Real World Scans" is accepted by JCISE.</li>
                <li>[Feb 2023] "The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects" is accepted by CVPR'23.</li>
                <li>[Jan 2023] "Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear" is accepted by ICRA'23.</li>
                <li>[Aug 2022] "See, Hear, Feel: Smart Sensory Fusion for Robotic Manipulation" is accepted by CoRL'22.</li> -->
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research
                ( <a href="#" id="filter-selected" onclick="showSelected(); return false;" class="filter-link active">Show Selected</a> |
                <a href="#" id="filter-all" onclick="showAll(); return false;" class="filter-link">Show All</a> )
              </heading>
              <p>
                I am interested in medical robotics, tactile sensing, soft robotics, and sim-to-real applications. Most of my research focuses on sensing-driven and learning-based approaches for safe interaction and navigation in medical and clinical environments.
              </p>
                          </td>
          </tr>
        </tbody></table>
    <div id="selected">

    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <p>
        <tr>
          <td style="padding:20px;width:30%;vertical-align:top">
            <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457"> <img style="width:107%;max-width:107%" src="images/ijrr.gif" class="hoverZoomLink">
            </a>
          </td>
          <td style="padding:20px;width:70%;vertical-align:top">
            <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">
            <!-- heading -->
            <papertitle>Gravity-Aware Proactive Joint-Level Compensation for Portable Soft Slender Robots Using A Single IMU and Real-Time Simulation</papertitle>
            </a>
            <!-- authors -->
            <br>
            Jiewen Lai*, <strong>Tian-Ao Ren*(Co-first author)</strong>, Pengfei Ye, Yanjun Liu, Jingyao Sun, Hongliang Ren
            <!-- conference & date -->
            <br>
            <em>International Journal of Robotics Research (IJRR)</em>, 2025
            <br>
            <!-- links -->
            <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">arXiv</a>
            <p></p>
            <p>
            <strong>TL;DR:</strong> Embodying gravity sensation in soft slender robots with a minimalist setup.
          </p>
          </td>
        </tr>

       
        <tr>
          <td style="padding:20px;width:30%;vertical-align:top">
            <a href="https://arxiv.org/abs/2512.20992"> <img style="width:107%;max-width:107%" src="images/DMD.png" class="hoverZoomLink">
            </a>
          </td>
          <td style="padding:20px;width:70%;vertical-align:top">
            <a href="https://arxiv.org/abs/2512.20992">
            <!-- heading -->
            <papertitle>Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation</papertitle>
            </a>
            <!-- authors -->
            <br>
            <strong>Tian-Ao Ren</strong>, Jorge Garcia, Seongheon Hong, Jared Grinberg, Hojung Choi, Julia Di, Hao Li, Dmitry Grinberg, Mark R. Cutkosky
            <!-- conference & date -->
            <br>
            <em>Under review by Design of Medical Device Conference (DMD)</em>, 2026
            <br>
            <!-- links -->
            <a href="https://arxiv.org/abs/2512.20992">arXiv</a>
            <p></p>
            <p>
            <strong>TL;DR:</strong> Combining vision-based tactile imaging with force-torque sensing enables robots to reliably detect subsurface tendon features during physiotherapy palpation, where force signals alone are often ambiguous, while still maintaining safe and controlled contact. 
          </p>
          </td>
        </tr>
  

        <tr>
          <td style="padding:20px;width:30%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/abstract/document/10171986"> <img style="width:107%;max-width:107%" src="images/Sim2Real.png" class="hoverZoomLink">
            </a>
          </td>
          <td style="padding:20px;width:70%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/abstract/document/10171986">
            <!-- heading -->
            <papertitle>Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision</papertitle>
            </a>
            <!-- authors -->
            <br>
            Jiewen Lai*, <strong>Tian-Ao Ren*(Co-first author)</strong>, Wenchao Yue, Shijian Su, Jason Chan, Hongliang Ren
            <!-- conference & date -->
            <br>
            <em>IEEE Transaction on Industrial Informatics(T-II)</em>, 2023
            <br>
            <!-- links -->
            <a href="https://youtu.be/vQ4xzFM9iwk">Supply Video</a>
              /
              <a href="https://ieeexplore.ieee.org/abstract/document/10171986">Paper</a>
             <p></p>
            <p>
            <strong>TL;DR:</strong> Transferring the navigation strategy that a redundant soft robot learns from what it has seen in the SOFA-based virtual world to the real world.
          </p>
          </td>
        </tr>

      </p>
    </tbody></table>
    </div>

  <div id="all" class="hidden">
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <p>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457"> <img style="width:107%;max-width:107%" src="images/nc.jpg" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">
          <!-- heading -->
          <papertitle>Twistable Soft Continuum Robots</papertitle>
          </a>
          <!-- authors -->
          <br>
          Jiewen Lai*, Yanjun Liu*, <strong>Tian-Ao Ren</strong>, Yan Ma, Tao Zhang, Jeremy Teoh, Mark R. Cutkosky, Hongliang Ren
          <!-- conference & date -->
          <br>
          <em>Second Round review by Nature Communications</em>, 2025
          <br>
          <!-- links -->
          <!-- <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">arXiv</a> -->
          <p></p>
          <p>
          <!-- <strong>TL;DR:</strong> Embodying gravity sensation in soft slender robots with a minimalist setup. -->
        </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457"> <img style="width:107%;max-width:107%" src="images/ijrr.gif" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">
          <!-- heading -->
          <papertitle>Gravity-Aware Proactive Joint-Level Compensation for Portable Soft Slender Robots Using A Single IMU and Real-Time Simulation</papertitle>
          </a>
          <!-- authors -->
          <br>
          Jiewen Lai, <strong>Tian-Ao Ren(Co-first author)</strong>, Pengfei Ye, Yanjun Liu, Jingyao Sun, Hongliang Ren
          <!-- conference & date -->
          <br>
          <em>International Journal of Robotics Research (IJRR)</em>, 2025
          <br>
          <!-- links -->
          <a href="https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4062970/1164457">arXiv</a>
          <p></p>
          <p>
          <strong>TL;DR:</strong> Embodying gravity sensation in soft slender robots with a minimalist setup.
        </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://arxiv.org/abs/2512.20992"> <img style="width:107%;max-width:107%" src="images/DMD.png" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://arxiv.org/abs/2512.20992">
          <!-- heading -->
          <papertitle>Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation</papertitle>
          </a>
          <!-- authors -->
          <br>
          <strong>Tian-Ao Ren</strong>, Jorge Garcia, Seongheon Hong, Jared Grinberg, Hojung Choi, Julia Di, Hao Li, Dmitry Grinberg, Mark R. Cutkosky
          <!-- conference & date -->
          <br>
          <em>Under review by Design of Medical Device Conference (DMD)</em>, 2026
          <br>
          <!-- links -->
          <a href="https://arxiv.org/abs/2512.20992">arXiv</a>
          <p></p>
          <p>
          <strong>TL;DR:</strong> Combining vision-based tactile imaging with force–torque sensing enables robots to reliably detect subsurface tendon features during physiotherapy palpation, where force signals alone are often ambiguous, while still maintaining safe and controlled contact. 
        </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/11176870"> <img style="width:107%;max-width:107%" src="images/tfr.gif" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/11176870">
          <!-- heading -->
          <papertitle>Gentle Grasping With Gecko-Inspired Adhesives in Extreme Environments</papertitle>
          </a>
          <!-- authors -->
          <br>
          Emj Rennich, <strong>Tian-Ao Ren</strong>, Jihyeon Kim, Julia Di, Tony G. Chen, Mark R. Cutkosky
          <!-- conference & date -->
          <br>
          <em>IEEE Transactions on Field Robotics (T-FR)</em>, 2025
          <br>
          <!-- links -->
          <!-- <a href="https://youtu.be/vQ4xzFM9iwk">Supply Video</a>
            /
            <a href="https://ieeexplore.ieee.org/abstract/document/10171986">Paper</a> -->
           <p></p>
          <p>
          <strong>TL;DR:</strong> Gecko-inspired dry adhesives enable gentle robotic grasping in extreme cold, fail below ~-60 °C, and can be reliably restored at much lower temperatures using brief local heating and appropriate preload.
        </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://arxiv.org/pdf/2509.00319"> <img style="width:107%;max-width:107%" src="images/robio2025.png" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://arxiv.org/pdf/2509.00319">
          <!-- heading -->
          <papertitle>Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach</papertitle>
          </a>
          <!-- authors -->
          <br>
          Chikit Ng, Huxin Gao, <strong>Tian-Ao Ren</strong>, Jiewen Lai, Hongliang Ren
          <!-- conference & date -->
          <br>
          <em> IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>, 2025
          <br>
          <!-- links -->
            <a href="https://arxiv.org/pdf/2509.00319">Best Paper Award</a>
           <p></p>
          <p>
          <strong>TL;DR:</strong> A force-informed deep reinforcement learning strategy enables flexible robotic endoscopes to exploit contact with deformable stomach walls for robust, high-precision navigation in dynamic environments, significantly outperforming contact-agnostic policies and generalizing to unseen disturbances.
        </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:45%;vertical-align:top">
          <a href="https://taccap.github.io/"><video style="width:107%;max-width:107%" autoplay muted loop class="hoverZoomLink"><source src="data/taccap.mp4" type="video/mp4"></video></a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://taccap.github.io/">
          <!-- heading -->
          <papertitle>TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot Skill Transfer</papertitle>
          </a>
          <!-- authors -->
          <br>
          Chengyi Xing*, Hao Li*, Yi-Lin Wei, <strong>Tian-Ao Ren</strong>, Tianyu Tu, Yuhao Lin, Elizabeth Schumann, Wei-Shi Zheng, Mark Cutkosky
          <note>(*Equal Contribution)</note>
          <!-- conference & date -->
          <br>
          <em>Intelligent Robots and Systems (IROS), Accepted</em>
          <br>
          <!-- links -->
          <a href="https://taccap.github.io/">project page</a>
          / <a href="https://arxiv.org/pdf/2503.01789">arXiv</a>
          <!-- / <a href="TBD">code</a> -->
          <p></p>
          <p>We present the design of FBG-based wearable tactile sensors capable of transferring tactile data collected by human hands to robotic hands.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/10907755"> <img style="width:107%;max-width:107%" src="images/robio2024.png" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/10907755">
          <!-- heading -->
          <papertitle>Three-dimensional Morphological Reconstruction of Millimeter-Scale Soft Continuum Robots based on Dual-Stereo-Vision</papertitle>
          </a>
          <!-- authors -->
          <br>
          <strong>Tian-Ao Ren</strong>, Wenyan Liu, Tao Zhang, Lei Zhao, Hongliang Ren, Jiewen Lai
          <!-- conference & date -->
          <br>
          <em> IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>, 2024
          <br>
          <!-- links -->
            <a href="https://ieeexplore.ieee.org/abstract/document/10907755">Paper</a>
           <p></p>
          <p>
          <strong>TL;DR:</strong> A dual stereo vision system with geometry-guided point-cloud relocation enables accurate 3D morphological reconstruction of millimeter-scale soft continuum robots, recovering fine notch-level details despite low-resolution depth sensing.
        </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://openreview.net/pdf?id=EfOuE6WHRd"> <img style="width:107%;max-width:107%" src="images/ARSO.png" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://openreview.net/pdf?id=EfOuE6WHRd">
          <!-- heading -->
          <papertitle>Navigation of tendon-driven flexible robotic endoscope through deep reinforcement learning</papertitle>
          </a>
          <!-- authors -->
          <br>
          Chikit Ng, Huxin Gao, <strong>Tian-Ao Ren</strong>, Jiewen Lai, Hongliang Ren
          <!-- conference & date -->
          <br>
          <em>IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO)</em>, 2024
          <br>
          <!-- links -->
            <a href="https://openreview.net/pdf?id=EfOuE6WHRd">Paper</a>
           <p></p>
          <p>
          <strong>TL;DR:</strong> A model-free deep reinforcement learning controller enables tendon-driven flexible endoscopes to autonomously navigate in both free space and contact-rich environments, achieving over 90% success within clinical accuracy by retraining policies learned in free space for contact scenarios.
        </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://link.springer.com/article/10.1007/s11517-023-02877-0"> <img style="width:107%;max-width:107%" src="images/domain.gif" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://link.springer.com/article/10.1007/s11517-023-02877-0">
          <!-- heading -->
          <papertitle>Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs</papertitle>
          </a>
          <!-- authors -->
          <br>
          Guankun Wang, <strong>Tian-Ao Ren</strong>, Jiewen Lai, Long Bai, and Hongliang Ren
          <!-- conference & date -->
          <br>
          <em>Medical & Biological Engineering & Computing(MBEC)</em>, 2023
          <br>
          <!-- links -->
            <a href="https://link.springer.com/article/10.1007/s11517-023-02877-0">Paper</a>
           <p></p>
          <p>
          <strong>TL;DR:</strong> A domain-adaptive Sim-to-Real framework combining IoU-guided image blending and style transfer enables accurate and stable segmentation of oropharyngeal organs from synthetic data, significantly improving real-world performance for robotic intubation despite limited real images.
        </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:30%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/10171986"> <img style="width:107%;max-width:107%" src="images/Sim2Real.png" class="hoverZoomLink">
          </a>
        </td>
        <td style="padding:20px;width:70%;vertical-align:top">
          <a href="https://ieeexplore.ieee.org/abstract/document/10171986">
          <!-- heading -->
          <papertitle>Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision</papertitle>
          </a>
          <!-- authors -->
          <br>
          Jiewen Lai, <strong>Tian-Ao Ren*(Co-first author)</strong>, Wenchao Yue, Shijian Su, Jason Chan, Hongliang Ren
          <!-- conference & date -->
          <br>
          <em>IEEE Transaction on Industrial Informatics(T-II)</em>, 2023
          <br>
          <!-- links -->
          <a href="https://youtu.be/vQ4xzFM9iwk">Supply Video</a>
            /
            <a href="https://ieeexplore.ieee.org/abstract/document/10171986">Paper</a>
           <p></p>
          <p>
          <strong>TL;DR:</strong> Transferring the navigation strategy that a redundant soft robot learns from what it has seen in the SOFA-based virtual world to the real world.
        </p>
        </td>
      </tr>

   
 
      </p>
  </tbody></table>
  </div>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Services</heading>
              <p>
                Reviewer for IROS 2024; ICRA 2024/2025/2026; IEEE Transactions on Medical Robotics and Bionics (T-MRB); IEEE Transactions on Industrial Informatics (T-II)
              <p>
            <!-- Organizer of <a href="https://stanfordasl.github.io/robotics_seminar/"> ENGR319: Stanford Robotics Seminar</a>, Stanford University, 2024 - Present -->
              </p>
            </td>
          </tr>
        </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Teaching</heading>
          <p>
            Course Assistant in <a href="https://web.stanford.edu/class/me327/">ME 327: Design and Control of Haptic Systems</a>, Stanford University, Spring 2025
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Honors & Awards</heading>
        <p>
          Best Paper Award, ROBIO 2025
        </p>
        <p>
          Best Poster Award, ICRA Workshop 2023 
        </p>
        <p>Outstanding Graduates in Beijing, 2023</p>
        <p>National Scholarship of China, 2022</p>
      </td>
    </tr>
  </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Template from Jon Barron's website</a>
              </p>
            </td>
            <td style="text-align:right;">
              <!-- <td style="padding:300px;width:15%;vertical-align:middle"> -->
              <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=2NmscI4iZxxfuoDbbJvZzjVOT14kVLh72R9xOD8Ww8g"></script>
            </td>
          </tr>
    </tbody></table>
  </table>

  <script>
    function showSelected() {
      console.log('Showing selected content');
      document.getElementById('selected').classList.remove('hidden');
      document.getElementById('all').classList.add('hidden');
      document.getElementById('filter-selected').classList.add('active');
      document.getElementById('filter-all').classList.remove('active');
    }

    function showAll() {
      console.log('Showing all content');
      document.getElementById('selected').classList.add('hidden');
      document.getElementById('all').classList.remove('hidden');
      document.getElementById('filter-selected').classList.remove('active');
      document.getElementById('filter-all').classList.add('active');
    }
  </script>
</body>

</html>
